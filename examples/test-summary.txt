ilo syntax comparison
==========================================================================================

  Idea                          Tokens   vs Py   Chars   vs Py   Score  Out tok
  --------------------------------------------------------------------------
  python-baseline                  871   1.00x    3635   1.00x       —        —  *
  idea1                            921   1.06x    3108   0.86x    10.0      344
  idea1-compact                    677   0.78x    2564   0.71x    10.0      225
  idea2-tool-calling               983   1.13x    3203   0.88x    10.0      360
  idea3-constrained-decoding       598   0.69x    2187   0.60x    10.0      220
  idea4-ast-bytecode               584   0.67x    1190   0.33x     9.8      318
  idea5-workflow-dag               710   0.82x    2603   0.72x    10.0      238
  idea6-mcp-composition            956   1.10x    2978   0.82x     9.5      316
  idea7-dense-wire                 351   0.40x    1292   0.36x    10.0      190
  idea8-ultra-dense                285   0.33x     901   0.25x    10.0       90
  idea9-ultra-dense-short          287   0.33x     787   0.22x    10.0       92

  Tokens  = total tokens across 5 examples (cl100k_base, comments stripped)
  Chars   = total characters
  Score   = LLM generation accuracy /10 (spec + all examples, claude-haiku-4-5)
  Out tok = avg output tokens generated
  * = baseline


Per-task breakdown (Full test)
==========================================================================================

  Idea                            workflow   data_pipe    decision    api_orch
  ------------------------------------------------------------------------
  idea1                              10.0       10.0       10.0       10.0
  idea1-compact                      10.0       10.0       10.0       10.0
  idea2-tool-calling                 10.0       10.0       10.0       10.0
  idea3-constrained-decoding         10.0       10.0       10.0       10.0
  idea4-ast-bytecode                 10.0       10.0        9.0       10.0
  idea5-workflow-dag                 10.0       10.0       10.0       10.0
  idea6-mcp-composition               9.0       10.0        9.0       10.0
  idea7-dense-wire                   10.0       10.0       10.0       10.0
  idea8-ultra-dense                  10.0       10.0       10.0       10.0
  idea9-ultra-dense-short            10.0       10.0       10.0       10.0


Benchmark: tot(10, 20, 30) — p*q + p*q*r = 6200
==========================================================================================

  ilo backends               Per call     vs Interpreter
  ----------------------------------------------------------
  Rust interpreter           1,602 ns          1.0x
  Register VM                  160 ns         10.0x
  Register VM (reused)         112 ns         14.3x
  Custom JIT (arm64)                       3 ns        534.0x

  External runtimes          Per call     vs Interpreter
  ----------------------------------------------------------
  CPython (equivalent)         226 ns          7.1x
  Python transpiled             88 ns         18.2x
  PyPy3 (JIT)                  125 ns         12.8x
  V8 / Node.js (JIT)            16 ns        100.1x
  LuaJIT                         2 ns        801.0x

  Notes:
  - 10,000 iterations, release build, Apple Silicon (aarch64)
  - Custom JIT (arm64) is 37.3x faster than VM (reusable), 29.3x faster than Python
  - Custom JIT (arm64) within striking distance of LuaJIT (3ns vs 2ns)
  - V8 JIT at 16ns — ilo Custom JIT (arm64) is 5.3x faster
  - Python competitive with VM due to CPython's optimized numeric path
  - Cranelift/LLVM JIT not enabled in this run
